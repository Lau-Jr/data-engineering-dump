{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "data eng staffs.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOtX6gvcDOKiohQx7auLwah",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lau-Jr/data-engineering-dump/blob/main/data_eng_staffs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tpVHMT400eWH"
      },
      "outputs": [],
      "source": [
        "# Function to extract table to a pandas DataFrame\n",
        "def extract_table_to_pandas(tablename, db_engine):\n",
        "    query = \"SELECT * FROM {}\".format(tablename)\n",
        "    return pd.read_sql(query, db_engine)\n",
        "\n",
        "# Connect to the database using the connection URI\n",
        "connection_uri = \"postgresql://repl:password@localhost:5432/pagila\" \n",
        "db_engine = sqlalchemy.create_engine(connection_uri)\n",
        "\n",
        "# Extract the film table into a pandas DataFrame\n",
        "extract_table_to_pandas(\"film\", db_engine)\n",
        "\n",
        "# Extract the customer table into a pandas DataFrame\n",
        "extract_table_to_pandas(\"customer\", db_engine)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "# Fetch the Hackernews post\n",
        "resp = requests.get(\"https://hacker-news.firebaseio.com/v0/item/16222426.json\")\n",
        "\n",
        "# Print the response parsed as JSON\n",
        "print(resp.json())####status_code\n",
        "\n",
        "# Assign the score of the test to post_score\n",
        "post_score = resp.json()[\"score\"]\n",
        "print(post_score)"
      ],
      "metadata": {
        "id": "fQ1MaW4-074J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests"
      ],
      "metadata": {
        "id": "fDVCu3dD3LBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resp = requests.get(\"https://hacker-news.firebaseio.com/v0/item/16222426.json\")\n",
        "resp.json()\n",
        "df = pd.read_csv(\"/content/sample_data/california_housing_test.csv\")"
      ],
      "metadata": {
        "id": "W0-1nDxb3Yq9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# spliting data in panda\n",
        "# df.assign()"
      ],
      "metadata": {
        "id": "1Q1LEcx74BGm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import SparkSession from pyspark.sql\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create my_spark\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# Print my_spark\n",
        "print(spark)"
      ],
      "metadata": {
        "id": "-SPqQxZWCZM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the tables in the catalog\n",
        "print(spark.catalog.listTables())"
      ],
      "metadata": {
        "id": "0W7cdvXNENKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Don't change this query\n",
        "query = \"FROM flights SELECT * LIMIT 10\"\n",
        "\n",
        "# Get the first 10 rows of flights\n",
        "flights10 = spark.sql(query)\n",
        "\n",
        "# Show the results\n",
        "flights10.show()"
      ],
      "metadata": {
        "id": "7lbhaTqSGVTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Don't change this query\n",
        "query = \"SELECT origin, dest, COUNT(*) as N FROM flights GROUP BY origin, dest\"\n",
        "\n",
        "# Run the query\n",
        "flight_counts = spark.sql(query)\n",
        "\n",
        "# Convert the results to a pandas DataFrame\n",
        "pd_counts = flight_counts.toPandas()\n",
        "\n",
        "# Print the head of pd_counts\n",
        "print(pd_counts)"
      ],
      "metadata": {
        "id": "LCVB_k22Nb0Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create pd_temp\n",
        "pd_temp = pd.DataFrame(np.random.random(10))\n",
        "\n",
        "# Create spark_temp from pd_temp\n",
        "spark_temp = spark.createDataFrame(pd_temp)\n",
        "\n",
        "# Examine the tables in the catalog\n",
        "print(spark.catalog.listTables())\n",
        "\n",
        "# Add spark_temp to the catalog\n",
        "spark_temp.createTempView('temp')\n",
        "\n",
        "# Examine the tables in the catalog again\n",
        "print(spark.catalog.listTables())"
      ],
      "metadata": {
        "id": "_Xbnmh87SF_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Don't change this file path\n",
        "file_path = \"/usr/local/share/datasets/airports.csv\"\n",
        "\n",
        "# Read in the airports data\n",
        "airports = spark.read.csv(file_path, header=True)\n",
        "\n",
        "# Show the data\n",
        "airports.show()"
      ],
      "metadata": {
        "id": "_uJDPGdWUdBR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}