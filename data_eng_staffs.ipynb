{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "data eng staffs.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNz7Mel5NHb2qNcMITenDmm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lau-Jr/data-engineering-dump/blob/main/data_eng_staffs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tpVHMT400eWH"
      },
      "outputs": [],
      "source": [
        "# Function to extract table to a pandas DataFrame\n",
        "def extract_table_to_pandas(tablename, db_engine):\n",
        "    query = \"SELECT * FROM {}\".format(tablename)\n",
        "    return pd.read_sql(query, db_engine)\n",
        "\n",
        "# Connect to the database using the connection URI\n",
        "connection_uri = \"postgresql://repl:password@localhost:5432/pagila\" \n",
        "db_engine = sqlalchemy.create_engine(connection_uri)\n",
        "\n",
        "# Extract the film table into a pandas DataFrame\n",
        "extract_table_to_pandas(\"film\", db_engine)\n",
        "\n",
        "# Extract the customer table into a pandas DataFrame\n",
        "extract_table_to_pandas(\"customer\", db_engine)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "# Fetch the Hackernews post\n",
        "resp = requests.get(\"https://hacker-news.firebaseio.com/v0/item/16222426.json\")\n",
        "\n",
        "# Print the response parsed as JSON\n",
        "print(resp.json())####status_code\n",
        "\n",
        "# Assign the score of the test to post_score\n",
        "post_score = resp.json()[\"score\"]\n",
        "print(post_score)"
      ],
      "metadata": {
        "id": "fQ1MaW4-074J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests"
      ],
      "metadata": {
        "id": "fDVCu3dD3LBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resp = requests.get(\"https://hacker-news.firebaseio.com/v0/item/16222426.json\")\n",
        "resp.json()\n",
        "df = pd.read_csv(\"/content/sample_data/california_housing_test.csv\")"
      ],
      "metadata": {
        "id": "W0-1nDxb3Yq9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# spliting data in panda\n",
        "# df.assign()"
      ],
      "metadata": {
        "id": "1Q1LEcx74BGm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import SparkSession from pyspark.sql\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create my_spark\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# Print my_spark\n",
        "print(spark)"
      ],
      "metadata": {
        "id": "-SPqQxZWCZM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the tables in the catalog\n",
        "print(spark.catalog.listTables())"
      ],
      "metadata": {
        "id": "0W7cdvXNENKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Don't change this query\n",
        "query = \"FROM flights SELECT * LIMIT 10\"\n",
        "\n",
        "# Get the first 10 rows of flights\n",
        "flights10 = spark.sql(query)\n",
        "\n",
        "# Show the results\n",
        "flights10.show()"
      ],
      "metadata": {
        "id": "7lbhaTqSGVTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Don't change this query\n",
        "query = \"SELECT origin, dest, COUNT(*) as N FROM flights GROUP BY origin, dest\"\n",
        "\n",
        "# Run the query\n",
        "flight_counts = spark.sql(query)\n",
        "\n",
        "# Convert the results to a pandas DataFrame\n",
        "pd_counts = flight_counts.toPandas()\n",
        "\n",
        "# Print the head of pd_counts\n",
        "print(pd_counts)"
      ],
      "metadata": {
        "id": "LCVB_k22Nb0Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create pd_temp\n",
        "pd_temp = pd.DataFrame(np.random.random(10))\n",
        "\n",
        "# Create spark_temp from pd_temp\n",
        "spark_temp = spark.createDataFrame(pd_temp)\n",
        "\n",
        "# Examine the tables in the catalog\n",
        "print(spark.catalog.listTables())\n",
        "\n",
        "# Add spark_temp to the catalog\n",
        "spark_temp.createTempView('temp')\n",
        "\n",
        "# Examine the tables in the catalog again\n",
        "print(spark.catalog.listTables())"
      ],
      "metadata": {
        "id": "_Xbnmh87SF_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Don't change this file path\n",
        "file_path = \"/usr/local/share/datasets/airports.csv\"\n",
        "\n",
        "# Read in the airports data\n",
        "airports = spark.read.csv(file_path, header=True)\n",
        "\n",
        "# Show the data\n",
        "airports.show()"
      ],
      "metadata": {
        "id": "_uJDPGdWUdBR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the DataFrame flights\n",
        "flights = spark.table('flights')\n",
        "\n",
        "# Show the head\n",
        "flights.show()\n",
        "\n",
        "# Add duration_hrs\n",
        "flights = flights.withColumn('duration_hrs',flights.air_time / 60)\n",
        "flights.show()"
      ],
      "metadata": {
        "id": "yNBAkk7Pet9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter flights by passing a string\n",
        "long_flights1 = flights.filter(\"distance > 1000\")\n",
        "\n",
        "# Filter flights by passing a column of boolean values\n",
        "long_flights2 = flights.filter(flights.distance > 1000)\n",
        "\n",
        "# Print the data to check they're equal\n",
        "long_flights1.show()\n",
        "long_flights2.show()"
      ],
      "metadata": {
        "id": "GKhT5xd4h61Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# spliting col\n",
        "# Get the rental rate column as a string\n",
        "rental_rate_str = film_df.rental_rate.astype(str)\n",
        "\n",
        "# Split up and expand the column\n",
        "rental_rate_expanded = rental_rate_str.str.split(\".\", expand=True)\n",
        "\n",
        "# Assign the columns to film_df\n",
        "film_df = film_df.assign(\n",
        "    rental_rate_dollar=rental_rate_expanded[0],\n",
        "    rental_rate_cents=rental_rate_expanded[1],\n",
        ")\n",
        "\n",
        "print(film_df.head())"
      ],
      "metadata": {
        "id": "J192KKP8i1Rz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# geting data to spark_df\n",
        "spark.read.jdbc(\"jdbc:postgresql://localhost:5432/pagila\",\n",
        "                \"customer\",\n",
        "                {\"user\":\"repl\",\"password\":\"password\"})"
      ],
      "metadata": {
        "id": "EgBMGAigjJWP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# groupby spark_df\n",
        "# Use groupBy and mean to aggregate the column\n",
        "ratings_per_film_df = rating_df.groupBy('film_id').mean('rating')\n",
        "\n",
        "# Join the tables using the film_id column\n",
        "film_df_with_ratings = film_df.join(\n",
        "    ratings_per_film_df,\n",
        "    film_df.film_id==rating_df.film_id\n",
        ")\n",
        "\n",
        "# Show the 5 first results\n",
        "print(film_df_with_ratings.show(5))"
      ],
      "metadata": {
        "id": "BAzol2R-kESI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Import a subset of columns"
      ],
      "metadata": {
        "id": "cHSHQzV8oEHC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create list of columns to use\n",
        "cols = ['zipcode','agi_stub','mars1','MARS2','NUMDEP']\n",
        "\n",
        "# Create dataframe from csv using only selected columns\n",
        "data = pd.read_csv(\"vt_tax_data_2016.csv\", usecols=cols)\n",
        "\n",
        "# View counts of dependents and tax returns by income level\n",
        "print(data.groupby(\"agi_stub\").sum())"
      ],
      "metadata": {
        "id": "PaL68UBKoOXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Import a file in chunks"
      ],
      "metadata": {
        "id": "hdmzPV8PpaoY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dataframe of next 500 rows with labeled columns\n",
        "vt_data_next500 = pd.read_csv(\"vt_tax_data_2016.csv\", \n",
        "                       \t\t  skiprows = 500,\n",
        "                       \t\t  nrows=500,\n",
        "                       \t\t  header = None,\n",
        "                       \t\t  names = list(vt_data_first500.columns))\n",
        "\n",
        "# View the Vermont dataframes to confirm they're different\n",
        "print(vt_data_first500.head())\n",
        "print(vt_data_next500.head())"
      ],
      "metadata": {
        "id": "lEc6I8DmpfP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Specify data types"
      ],
      "metadata": {
        "id": "LdXcDMfky1D_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dict specifying data types for agi_stub and zipcode\n",
        "data_types = {'agi_stub':'category',\n",
        "\t\t\t  'zipcode':str}\n",
        "\n",
        "# Load csv using dtype to set correct data types\n",
        "data = pd.read_csv(\"vt_tax_data_2016.csv\", dtype = data_types)\n",
        "\n",
        "# Print data types of resulting frame\n",
        "print(data.dtypes.head())"
      ],
      "metadata": {
        "id": "RFXiGXUby39-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Set custom NA values"
      ],
      "metadata": {
        "id": "zKZASnT31alS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dict specifying that 0s in zipcode are NA values\n",
        "null_values = {'zipcode':0}\n",
        "\n",
        "# Load csv using na_values keyword argument\n",
        "data = pd.read_csv(\"vt_tax_data_2016.csv\", \n",
        "                   na_values = null_values)\n",
        "\n",
        "# View rows with NA ZIP codes\n",
        "print(data[data.zipcode.isna()])"
      ],
      "metadata": {
        "id": "Y2gTKTxH1cHD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "QJAGfSaI1eqq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Skip bad data\n"
      ],
      "metadata": {
        "id": "40edfQzW1uui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  # Import the CSV without any keyword arguments\n",
        "  data = pd.read_csv('vt_tax_data_2016_corrupt.csv')\n",
        "  \n",
        "  # View first 5 records\n",
        "  print(data.head())\n",
        "  \n",
        "except pd.errors.ParserError:\n",
        "    print(\"Your data contained rows that could not be parsed.\")"
      ],
      "metadata": {
        "id": "Nuc7KADN1v1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  # Import CSV with error_bad_lines set to skip bad records\n",
        "  data = pd.read_csv(\"vt_tax_data_2016_corrupt.csv\", \n",
        "                     error_bad_lines = False)\n",
        "  \n",
        "  # View first 5 records\n",
        "  print(data.head())\n",
        "  \n",
        "except pd.errors.ParserError:\n",
        "    print(\"Your data contained rows that could not be parsed.\")"
      ],
      "metadata": {
        "id": "zVI4bp142REn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  # Set warn_bad_lines to issue warnings about bad records\n",
        "  data = pd.read_csv(\"vt_tax_data_2016_corrupt.csv\", \n",
        "                     error_bad_lines=False, \n",
        "                     warn_bad_lines = True)\n",
        "  \n",
        "  # View first 5 records\n",
        "  print(data.head())\n",
        "  \n",
        "except pd.errors.ParserError:\n",
        "    print(\"Your data contained rows that could not be parsed.\")"
      ],
      "metadata": {
        "id": "SJHMcXHe2bXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###IMPORTING EXCEL"
      ],
      "metadata": {
        "id": "PQO0dCpk4wId"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pandas as pd\n",
        "import pandas as pd\n",
        "\n",
        "# Read spreadsheet and assign it to survey_responses\n",
        "survey_responses = pd.read_excel('fcc_survey.xlsx')\n",
        "\n",
        "# View the head of the dataframe\n",
        "print(survey_responses.head())"
      ],
      "metadata": {
        "id": "WwBgABVT400m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###LOADING PORTION OF SPREADSHEET"
      ],
      "metadata": {
        "id": "TUz3nalk5pqZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create string of lettered columns to load\n",
        "col_string = 'AD,AW:BA'\n",
        "\n",
        "# Load data with skiprows and usecols set\n",
        "survey_responses = pd.read_excel(\"fcc_survey_headers.xlsx\", \n",
        "                        usecols = col_string, \n",
        "                        skiprows = 2)\n",
        "\n",
        "# View the names of the columns selected\n",
        "print(survey_responses.columns)"
      ],
      "metadata": {
        "id": "BAJrENgf5vWh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###PANDAS GROUP BY PRACTICE"
      ],
      "metadata": {
        "id": "fX_EdyVzXLib"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "ipl_data = {'Team': ['Riders', 'Riders', 'Devils', 'Devils', 'Kings',\n",
        "   'kings', 'Kings', 'Kings', 'Riders', 'Royals', 'Royals', 'Riders'],\n",
        "   'Rank': [1, 2, 2, 3, 3,4 ,1 ,1,2 , 4,1,2],\n",
        "   'Year': [2014,2015,2014,2015,2014,2015,2016,2017,2016,2014,2015,2017],\n",
        "   'Points':[876,789,863,673,741,812,756,788,694,701,804,690]}\n",
        "\n",
        "data = pd.DataFrame (ipl_data)"
      ],
      "metadata": {
        "id": "DoCwXuzVXTfs"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# apply fnx\n",
        "\n",
        "\n",
        "def to_upper(team):\n",
        "  return team.capitalize()\n",
        "\n",
        "data['Team'] = data['Team'].apply(to_upper)"
      ],
      "metadata": {
        "id": "uAlDdX7BY7aJ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grouped = data.groupby(['Team'])"
      ],
      "metadata": {
        "id": "-HrNhnghYf_T"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for x,y in grouped:# here grouped is not DF\n",
        "  print(x)\n",
        "  print(y)\n",
        "  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RAhsdasgZ55K",
        "outputId": "b07f3210-7bbf-45ad-8121-5907579aad36"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Devils\n",
            "     Team  Rank  Year  Points\n",
            "2  Devils     2  2014     863\n",
            "3  Devils     3  2015     673\n",
            "Kings\n",
            "    Team  Rank  Year  Points\n",
            "4  Kings     3  2014     741\n",
            "5  Kings     4  2015     812\n",
            "6  Kings     1  2016     756\n",
            "7  Kings     1  2017     788\n",
            "Riders\n",
            "      Team  Rank  Year  Points\n",
            "0   Riders     1  2014     876\n",
            "1   Riders     2  2015     789\n",
            "8   Riders     2  2016     694\n",
            "11  Riders     2  2017     690\n",
            "Royals\n",
            "      Team  Rank  Year  Points\n",
            "9   Royals     4  2014     701\n",
            "10  Royals     1  2015     804\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# getting a group \n",
        "kings = grouped.get_group('Kings')\n",
        "\n",
        "print(kings[['Year','Rank','Points']])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eq-qeCaMey0_",
        "outputId": "b918fc4e-bbc0-4681-94a9-beffeeaf4338"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Year  Rank  Points\n",
            "4  2014     3     741\n",
            "5  2015     4     812\n",
            "6  2016     1     756\n",
            "7  2017     1     788\n"
          ]
        }
      ]
    }
  ]
}